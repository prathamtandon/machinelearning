\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage{graphicx,amssymb,hyperref,stmaryrd}
\usepackage{wasysym,multirow}
\usepackage{algorithm,algorithmic,color}

\newcommand{\assignment}[4]{
\thispagestyle{plain} 
\newpage
\setcounter{page}{1}
\noindent
\begin{center}
\framebox{ \vbox{ \hbox to 6.28in
{\bf CS6140: Machine Learning \hfill Fall 2015}
\vspace{4mm}
\hbox to 6.28in
{\hspace{2.5in}\large\mbox{Problem Set #1}}
\vspace{4mm}
\hbox to 6.28in
{{\hfill {\em Official Due Date: #3}}}
\hbox to 6.28in
{{\it Handed Out: #2 \hfill Extended Due Date: #4}}
}}
\end{center}
}

\newcommand{\solution}[3]{
\thispagestyle{plain} 
\newpage
\setcounter{page}{1}
\noindent
\begin{center}
\framebox{ \vbox{ \hbox to 6.28in
{\bf CS6140: Machine Learning \hfill Fall 2015}
\vspace{4mm}
\hbox to 6.28in
{\hspace{2.5in}\large\mbox{Problem Set #1 Solutions}}
\vspace{4mm}
\hbox to 6.28in
{{\bf #2 \hfill Submitted: #3}}
}}
\end{center}
}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}

\assignment{3}{October 19, 2015}{\red{November 6, 2015}}{\red{November 10, 2015}}

\begin{footnotesize}
\begin{itemize}

\item
Please submit your solutions via your CCIS {\tt github} account.   

\item
Materials associated with this problem set are available at\\ \url{https://github.ccs.neu.edu/cs6140-02-fall2015/kevinsmall}.

\item
I encourage you to discuss the homework with other members of the class. The goal of the homework is for you to learn the course material. However, you should write your own solution.

\item
Please keep your solution brief, clear, and legible.  If you are feeling generous, I would {\em really} appreciate typed solutions (and if you plan on publishing CS/Math/Engineering research, this is actually a good exercise) -- see the source materials if you would like to use \LaTeX{} to do this.

\item
I encourage you to ask questions before class, after class, via email, or the Piazza QA section. However, please do not start e-mailing me questions the night before the homework is due. $\smiley$

\item
\red{Note that while the deadline is after the midterm on 11/3/15, you are still responsible for this material on the midterm.  Therefore, I recommend you at least sketch out your solutions before the midterm such that you understand the material.}

\end{itemize}
\end{footnotesize}

\begin{enumerate}
\item
{\bf [Expectation Maximization - 50 points]}

Suppose that a trove of Victorian era literature has recently been discovered and scholars believe that these works were intended to be compiled into an anthology of previously unpublished novels.  Unfortunately, the author names are missing and therefore are difficult to attribute.\footnote{Who would have thought that Victorian novelists invented the blind review process?}  For simplicity, we will assume that the length of each novel chapter is distributed according to a Poisson distribution parameterized by the single parameter $\lambda$.  More explicitly, for a non-negative integer $x$,
%
$$p(\mbox{wordcount}=x ; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}$$
%
where a parameter $\lambda_A$ is associated with each author.

\begin{enumerate}
\item
{\bf [7 points]}  Given a particular novel by author $A$, let $x_i$ denote the length of chapter $i$.  What is the maximum likelihood value of $\lambda_A$?
%
\item
{\bf [10 points]}  Good news; scholars have determined that each of these novels were written by either Thackeray or Trollope.  Assume that you are now provided with a corpus $\mathcal{C}$ of $m$ novels $\left\{ (x_1, \ldots, x_n)_j \right\}_{j=1}^m$ but do not know which novel comes from each of the aforementioned authors.  \red{Denote the probability of a novel being} generated by Thackeray is $\eta$.  Provide a generative story for the resulting data collection.\footnote{Admittedly, this is an unrealistic model as we are ignoring the actual text. C'est la vie.}  Make sure you name the parameters that are required to fully specify the model.

\item
{\bf [8 points]}  If I was to give you the parameters of the defined model, how would you use it to cluster issues into two groups -- specifically \red{into} the Thackeray and Trollope novels?

\item
{\bf [15 points]}  Given the corpus $\mathcal{C}$ of $m$ anonymized novels, specify the update rule of the EM algorithm.   Please show all of your work.

\item
{\bf [10 points]}  Write down the pseudocode for learning the model via EM.  Clearly define and delineate the initialization iteration, and termination steps including which equation(s) are used at each step.
\end{enumerate}

\item
{\bf [Logistic Regression -- 50 points]}

For this problem, you will be implementing stochastic gradient descent for text classification with logistic regression.  Specifically, we will be considering the email folder classification problem based on the Enron email dataset from Problem Set 2.  However, to simplify the problem, we will be attempting to automatically classify her emails into one of two folders, {\tt personal} or {\tt corporate}.

In the course github repository, you will again observe the {\tt lokay-m} folder which contains the original emails as processed by Ron Bekkerman.  Furthermore, in the {\tt libsvm} folder, I have further processed the data \red{into {\tt libsvm} format,}\footnote{I have adopted this standard as it is reasonably widely used -- see \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/} for more information} all of which was done with {\tt ProcessEmail.java} (also in the github repository) to generate sparse feature vectors.  Basically, each feature has an id followed by a superfluous {\tt 1.0} to indicate a {\em strength} of 1.  If you would like to see what the ids correspond to, look at {\tt features.lexicon}.\footnote{If you do look at this, it is easy to see that there is room for better preprocessing.}

Based on this process, you will find one training file and one test file.
As previously, the basic preprocessing algorithm was to compile the entire {\tt Lokay} corpus, remove end of sentence periods and oxford-style commas, lowercase the corpus, and split on spaces (see {\tt process\_line} if you want to change this for some reason).  Furthermore, I removed all tokens which did not occur at least three times and the 100 most frequent tokens (in a modest effort to remove determiners and the such).  Training data was all emails before 2002 and testing data was all emails in 2002.  The distribution of emails per folder is given below.

\begin{verbatim}
corporate training:362 testing:45
personal training:159 testing:31
\end{verbatim}

As previously stated, for this problem, you will be generating a Logistic Regression classifier with parameters estimated via Stochastic Gradient Descent (SGD) as given by Algorithm~\ref{alg:lr}.

\begin{algorithm}
\begin{algorithmic}
   \STATE {\bfseries Input:} Labeled training corpus $\mathcal{S} \subset \mathcal{X} \times \mathcal{Y}$ (where $\mathcal{X} \subset \mathbb{R}^D$ is the feature space and  $\mathcal{Y} \in \{0,1\}$ is the label space); learning rate $\alpha$; number of rounds $T$
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE $\mathbf{w} \leftarrow \mathbf{0}, w_0 \leftarrow 0$
   \COMMENT{$w_0$ is a standard convention for learning a {\em bias}}
   \STATE \COMMENT{Should pick your own halting criteria, but hard-coding rounds will work}
   \FOR {$t = 1, \ldots, T$}
      \STATE $\textsc{Shuffle}(\mathcal{S})$
      \FOR {$(\mathbf{x},y) \in \mathcal{S}$}
         \STATE $\sigma(\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^T\mathbf{x} - w_0)}$
         \COMMENT{Sigmoid function}
         \STATE $\delta \leftarrow y - \sigma(\mathbf{x})$
         \STATE \COMMENT{$D$ is dimensionality}
         \FOR {$d \leftarrow 1, \ldots, D$}
            \STATE $w_d \leftarrow w_d + \delta x_d$
         \ENDFOR
         \STATE $w_0 \leftarrow w_0 + \delta$ 
      \ENDFOR
   \ENDFOR
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE {\bfseries Output:} Learned parameters $\mathbf{w}$ and $w_0$ 
\caption{(Binary) Logistic Regression with Stochastic Gradient Descent}
\label{alg:lr}
\end{algorithmic}
\end{algorithm}

\clearpage
Once you have learned the parameters, the decision rule is given by

$$\hat{y} \leftarrow \llbracket  \sigma(\mathbf{x}) \geq 0.5 \rrbracket$$

where $\llbracket p \rrbracket = 1$ iff $p$ is true.  Since you are using SGD, you should randomize the order for every round of training, set the learning rate to a reasonable value, and possibly use a hold-out set to determine convergence.

\begin{enumerate}
\item
Create a program that can be run with the command

{\tt ./lr-run}

which should produce a predictions file {\tt predictions.lr} such that the labels should be mapped as stated int Table~\ref{table:mapping}.

\begin{table}[htb]
\centering
\begin{tabular}{|l|c|}
\hline
Text Label & Numerical Value \\
\hline
corporate & 2.0 \\
personal & 6.0 \\
\hline
\end{tabular}
\caption{Label to Numerical Value Mapping}
\label{table:mapping}
\end{table}

While this labeling might seem a bit strange, it will allow us to maintain consistency {\em if} I decide to use this data in future problem sets.  Look at {\tt output/labels.txt} to see the ``gold" labels in the decided format.  Additionally, in the {\tt output} directory, you can run {\tt evaluate.pl labels.txt predictions.lr} can be used to generate a confusion matrix (and {\tt predictions.lr} is the file generated above). 

\item
Describe anything you did differently in regards to processing the files or anything else we may find interesting.  I am particularly interested in how you set the learning rate and number of rounds. Note that you are allowed to use the files {\em as-is} and receive full credit.  However, I am always impressed by interesting results.

\item
Write down the confusion matrix for the logistic regression output.

\item
Interpret these results.

\item
Use your CCIS github repository to submit all relevant files.  You are free to use the programming language of your choice, but please attempt to conform to the instructions above.  To be safe, try submitting something {\bf before} the assignment deadline.

The code you submit must be your own. If you find/use information about specific algorithms from the Web, etc., be sure to cite the source(s) clearly in your source code.  You are not allowed to submit existing logistic regression implementations or code downloaded from the internet (obviously).

\end{enumerate}

\end{enumerate}

\end{document}